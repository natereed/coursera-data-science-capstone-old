---
title: "Analyzing a Natural Language Text Dataset: Updated Milestone Report"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache=TRUE)
```

## Overview

Coursera's Data Science Capstone project involves analyzing a text corpus from SwiftKey for the purpose of developing a method of predicting the next word in a sequence of text. 

## Recap

The author was on hiatus for several months. I decided to re-do the initial cleaning and text processing work. The initial results, published originally in March 2016, were incomplete.

Performance was a big problem in the initial attempt at developing a predictive model. The *tm* library was used for analysis, but it quickly became apparent that this library does not scale well for large data sets. Early attempts ended in frustration, as loading even a significant subset of the data gobbled up all available RAM and resulted in long running times. 

While it is possible to only use a small subset to build a model, the early version of the model was not very accurate. In order to get more accuracy, I want to process a larger subset of the corpus. There is a tradeoff between accuracy and space/time cost, so finding an efficient algorithm is important. My intuition is that a larger training data set will result in a more accurate model. This is something I will test systematically once I have successfully ingested the data and used it to build a predictive model.

I researched several options, from using large instances on EC2, to trying different libraries, such as distributed TM (which runs on a Hadoop cluster), quanteda (a fast text processing library) and text2vec. Ultimately, I chose to work with text2vec because it was engineered to process a large corpus while being memory-efficient. The API is also very use to use.

We will also use some functions from the *tm* library for pre-processing.

## Load Libraries

```{r, warning=FALSE, message=FALSE}
library(curl)
library(text2vec)
library(tm)
library(dplyr)
```

## Get the Data

```{r, eval=FALSE}
#setwd( file.path("C:/", "Users", "Owner", "Projects", "Coursera", "Data Science Capstone"))

#download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", destfile="Coursera-SwiftKey.zip")
#unzip(file.path("C:/", "Users", "Owner", "Projects", "Coursera", "Data Science Capstone", "Coursera-SwiftKey.zip"))
```

## Data Statistics

```{r load-data, cache=FALSE, warning=FALSE, message=FALSE}
setwd( file.path("C:/", "Users", "Owner", "Projects", "Coursera", "Data Science Capstone"))

dir <- file.path("final", "en_US")
news_path <- file.path(dir, "en_US.news.txt")
blogs_path <- file.path(dir, "en_US.blogs.txt")
twitter_path <- file.path(dir, "en_US.twitter.txt")

news_data <- readLines(news_path, encoding='UTF-8')
blogs_data  <- readLines(blogs_path, encoding='UTF-8')
twitter_data    <- readLines(twitter_path, encoding='UTF-8')

file_size <- c(file.info(news_path)[1, c('size')],
               file.info(blogs_path)[1, c('size')],
               file.info(twitter_path)[1, c('size')]);

stats <- data.frame(name=c('news', 'blogs', 'twitter'), 
                    bytes=file_size, 
                    num_lines=c(length(news_data),
                                length(blogs_data),
                                length(twitter_data)))
stats['gb'] <- stats['bytes'] / 1073741824
stats['percent'] <- stats['bytes'] / sum(stats['bytes'])
stats
```

We see that blogs are the largest source of data overall, although twitter has more lines, owing to the limit of 40 characters for tweets.

```{r}
library(ggplot2)
bp <- ggplot(stats, aes(x="", y=gb, fill=name)) + geom_bar(width=1, stat="identity")
pie <- bp + coord_polar("y", start=0)
pie
```

# Subset the Data

```{r subset-data, cache=FALSE}
sample_size <- 0.05
set.seed(1234)
news_subset <- sample(news_data, length(news_data) * sample_size)
rm(news_data)

blogs_subset <- sample(blogs_data, length(blogs_data) * sample_size)
rm(blogs_data)

twitter_subset <- sample(twitter_data, length(twitter_data) * sample_size)
rm(twitter_data)

combined_doc <- sample(c(news_subset, blogs_subset, twitter_subset))
rm(news_subset, blogs_subset, twitter_subset)
```

The variable combined_doc contains the subsetted data for the three sources: twitter, news and blogs.

## Clean the Data

### Remove unusual characters

The corpus contains many unusual characters, such as "control" characters, Latin characters, emoji's, etc. We read the files in as UTF-8 encoded. Text2vec cannot process these characters, and attempting to create a vocabulary from the raw text will result in errors. To faciliate our analysis, we will remove every character that is not an English character, whitespace or punctuation, by converting from UTF-8 to ASCII characters:

```{r}
combined_doc <- combined_doc <- iconv(combined_doc, 'utf-8', 'ascii', sub='')
```

### Remove Unwanted Text

The corpus contains character sequences that we don't necessarily want to include in our analysis. 

Numbers are not particularly useful, so we will use the *tm* package function *removeNumbers* in our pre-processing.

Twitter data contains hashtags (#) and @ tags. While these are not always useful for analyzing sequences of words, in some cases these are used as grammatical elements of the sentence. For example, one of the tweets is "que tal amigos? how are you guys? been following your work! congrats! #interview soon?" Another contains the text "Go #Canucks!"

The text "#interview" is a hashtag, which is used for searching. In this sentence, it actually serves as part of a question, as in "Do you have an interview soon?" or "Is there an interview soon?" I choose to leave the text "interview" but remove the hashtag character (#). 

Url's are also commonly found in tweets and other sources. We'll want to strip those out. We will also want to strip out profanity. For this, we can use a source like this one: http://www.bannedwordlist.com/lists/swearWords.txt 

### Convert Case

We will also standardize the text by converting all text to lowercase.

### No Stemming

Something to consider is whether to stem words. The root of the plural "cats," for example, is "cat." Stemming also changes the tense of English verbs. "Arguing" or "argued" becomes "argu" (the actual stem, which is not itself a word). See  https://en.wikipedia.org/wiki/Stemming for further explanation.

I decided not to stem words, as this would result in non-sensical or grammatically incorrect sequences such as "Yesterday we argu" or "We had agree on a solution."

## Contractions and Stop Words

Similarly, I decided to keep stop words and contractions. Stop words are very commonly used. Contractions, such as "can't" and "didn't", are also common, and we want to be able to predict words based on their relative frequencies in the corpus.

For this reason, we will also keep most punctuation, since contractions use apostrophes. Other punctuation, such as hyphens, might be meaningful. I will rely on text2vec to remove punctuation that marks the end of sentences, or separates compound sentences (periods and commas), as part of the tokenization process.

## The Code:

```{r clean-data, cache=FALSE}

clean_twitter <- function(t) {
  # Remove hashes
  t <- gsub('#', '', t)
  
  # Remove email addresses
  t <- gsub("[[:alnum:]]+\\@[[:alpha:]]+\\.com", '', t)
  
  # Remove url's
  t <- gsub('http\\S+\\s*', '', t)
  
  # Translate some common "textese" (a.k.a. "SMS language")
  # The list of such words is very long. TBD: Add comprehensive translation.
  t <- gsub('\\br\\b', 'are', t)
  t <- gsub('\\b@\\b', 'at', t)
  t <- gsub('\\bjst\\b', 'just', t)
  t <- gsub('\\bluv\\b', 'love', t)
  t <- gsub('\\bu\\b', 'you', t)
  return(t)
}

# Get swearwords
con <- curl(url="http://www.bannedwordlist.com/lists/swearWords.txt")
swear_words <- readLines(con)

strip_profanity_from_entry <- function(v) {
  expr <- do.call(paste, c(as.list(swear_words), sep="|"))
  return(v[grep(expr, v, invert=TRUE)])
}

strip_profanity <- function(vector_list) {
  return(lapply(vector_list, strip_profanity_from_entry))
}

# Chain functions for tokenizing and cleaning:
cleaning_tokenizer <- function(v) {
  v %>% 
    clean_twitter %>%
    removeNumbers %>%
    word_tokenizer %>%
    strip_profanity
} 
```

# Analyzing term frequencies

Now that we have a function to clean up the vocabulary, we can analyze the top terms. Note that iterators cannot be reused. We have to create a new iterator each time we build the vocabulary:

## Plotting code

```{r define-plotting-functions, cache=FALSE}
plot_terms <- function(top_terms) {
  p <- ggplot(top_terms, aes(x=factor(terms), y=terms_counts))
  return(p + geom_bar(stat="identity") + coord_flip())
}

```

## Unigrams
```{r}

# Create iterator -- can only be used once
it <- itoken(combined_doc, 
            preprocess_function = tolower, 
            tokenizer = cleaning_tokenizer);

# Create vocab from iterator
v <- create_vocabulary(it, ngram=c(1L, 1L));
top_terms <- arrange(v$vocab, desc(terms_counts))[1:20,]
plot_terms(top_terms)
```

## Bi-grams
```{r}
it <- itoken(combined_doc, 
            preprocess_function = tolower, 
            tokenizer = cleaning_tokenizer);

# Create vocab from iterator
v <- create_vocabulary(it, ngram=c(2L, 2L));
top_terms <- arrange(v$vocab, desc(terms_counts))[1:20,]
plot_terms(top_terms)
```

## Tri-grams
```{r}
it <- itoken(combined_doc, 
            preprocess_function = tolower, 
            tokenizer = cleaning_tokenizer);

# Create vocab from iterator
v <- create_vocabulary(it, ngram=c(3L, 3L));
top_terms <- arrange(v$vocab, desc(terms_counts))[1:20,]
plot_terms(top_terms)
```
